{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BpLVynUMhMH6"
      },
      "outputs": [],
      "source": [
        "# Mengimpor TensorFlow, modul utama untuk pengembangan model machine learning.\n",
        "import tensorflow as tf\n",
        "# Mengimpor NumPy, library yang digunakan untuk operasi numerik.\n",
        "import numpy as np\n",
        "# Mengimpor os, modul yang digunakan untuk berinteraksi dengan sistem operasi (contoh: membaca file, mengatur direktori, dll.).\n",
        "import os\n",
        "# Mengimpor time, modul yang digunakan untuk mengukur waktu atau menunda eksekusi program.\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan tf.keras.utils.get_file untuk mengunduh file teks dari URL yang diberikan.\n",
        "# Nama file yang diunduh adalah 'shakespeare.txt', dan URL sumbernya adalah 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'.\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "9b1zmjlWhsFQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "\n",
        "# Membaca isi file teks ('shakespeare.txt') yang telah diunduh dalam mode 'rb' (read binary),\n",
        "# lalu mendekodekannya dengan encoding 'utf-8' untuk mendapatkan teks dalam bentuk string.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Menghitung panjang teks, yaitu jumlah karakter dalam teks, lalu mencetaknya.\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDvJiIPFhyAb",
        "outputId": "537b23c0-d1a0-4721-ff8a-48634b159110"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengambil 250 karakter pertama dalam teks dan mencetaknya.\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwfnqSkfh5Et",
        "outputId": "7655d5ee-33cd-41b7-ae56-ae826ec891c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan set(text) untuk mengidentifikasi karakter-karakter unik dalam teks, lalu mengurutkannya dengan sorted.\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "# Mencetak jumlah karakter unik yang ditemukan dalam teks.\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n23ZWF5OiA7Z",
        "outputId": "b251be00-1842-48aa-a255-0746da039238"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Olah Teks\n",
        "\n",
        "# Mendefinisikan contoh teks dalam list 'example_texts'.\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "# Menggunakan tf.strings.unicode_split untuk membagi teks dalam contoh teks menjadi karakter-karakter individual.\n",
        "# input_encoding='UTF-8' menunjukkan bahwa encoding yang digunakan adalah UTF-8.\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "# Menampilkan hasil pemisahan karakter.\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgXayghLiCaa",
        "outputId": "da4d6a22-f27a-44ad-fa00-ba8e9f4f6aa5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat lapisan StringLookup dengan parameter-parameter berikut:\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab),  # Menggunakan daftar karakter unik sebagai vocabulary.\n",
        "    mask_token=None  # Tidak ada token mask yang digunakan.\n",
        ")"
      ],
      "metadata": {
        "id": "n6Iatun7iSra"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan lapisan StringLookup (ids_from_chars) untuk mengonversi karakter-karakter (chars) menjadi ID numerik.\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "# Menampilkan hasil konversi karakter ke ID.\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNC7k71BiaJh",
        "outputId": "df00ceb1-fa47-4f41-c6a8-3168aba6c8cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat lapisan StringLookup baru dengan parameter-parameter berikut:\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),  # Menggunakan vocabulary dari lapisan ids_from_chars.\n",
        "    invert=True,  # Mengindikasikan bahwa konversi dari ID ke teks akan dilakukan.\n",
        "    mask_token=None  # Tidak ada token mask yang digunakan.\n",
        ")"
      ],
      "metadata": {
        "id": "glm2fY93imVf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan lapisan StringLookup (chars_from_ids) untuk mengonversi ID numerik (ids) menjadi karakter teks.\n",
        "chars = chars_from_ids(ids)\n",
        "\n",
        "# Menampilkan hasil konversi ID ke karakter teks.\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJLTZsrCiwT3",
        "outputId": "14f8db3e-ae33-47eb-d338-0e06506b587b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan tf.strings.reduce_join untuk menggabungkan karakter-karakter dalam tensor chars menjadi teks lengkap.\n",
        "# axis=-1 menunjukkan bahwa penggabungan dilakukan sepanjang sumbu terakhir (axis terakhir).\n",
        "\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P16COnUix3a",
        "outputId": "b39334a6-e4bb-4a5d-f753-1fc10ded6e7a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendefinisikan sebuah fungsi text_from_ids yang mengambil ID numerik sebagai input.\n",
        "def text_from_ids(ids):\n",
        "    # Menggunakan chars_from_ids untuk mengonversi ID menjadi karakter-karakter teks, lalu menggabungkannya menjadi teks lengkap.\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "JR2HKTMajBZt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan lapisan ids_from_chars untuk mengonversi seluruh teks menjadi ID numerik\n",
        "# dengan memisahkan teks menjadi karakter-karakter terlebih dahulu.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# Mengonversi ID numerik menjadi objek tf.data.Dataset.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "# Mengulangi dataset untuk mengambil 10 data ID dan mencetak karakter teks yang sesuai dengan ID-nya.\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "# Menetapkan panjang urutan (sequence length) menjadi 100.\n",
        "seq_length = 100"
      ],
      "metadata": {
        "id": "p5dJZsldjGBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517d572f-878b-42e7-823f-c8e7c626a547"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memisahkan dataset ID numerik menjadi urutan dengan panjang (sequence length) sesuai dengan 'seq_length' + 1.\n",
        "# drop_remainder=True mengindikasikan bahwa jika ada sisa data yang kurang dari panjang urutan, data tersebut akan dihapus.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Mengambil urutan pertama dari dataset yang telah dibuat dan mencetak karakter teks yang sesuai dengan ID-nya.\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "i3dPYP27kEtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bdebeae-1e4e-4b5d-af55-9b8bf3c2fbd6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterasi melalui lima urutan pertama dalam dataset dan mencetak teks yang sesuai dengan ID-nya.\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())\n",
        "\n",
        "# Membuat fungsi split_input_target yang digunakan untuk membagi urutan menjadi input dan target.\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Membagi urutan dalam list \"Tensorflow\" menjadi input dan target, kemudian mencetaknya.\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "\n",
        "# Membangun dataset dengan urutan input dan target menggunakan fungsi split_input_target.\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Mengambil satu contoh dari dataset, kemudian mencetak input dan targetnya.\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "q_hHQC7xkFim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2019edd3-d434-49c6-eb96-c4b97c380f2d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menetapkan ukuran batch yang akan digunakan dalam pelatihan model.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size digunakan untuk mengatur sejauh mana dataset akan di-shuffle.\n",
        "# Ini penting karena TensorFlow data dirancang untuk bekerja dengan urutan yang mungkin tak berujung,\n",
        "# sehingga buffer ini digunakan untuk mengacak elemen-elemen dataset.\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Melakukan operasi shuffle pada dataset dengan menggunakan buffer sebesar BUFFER_SIZE.\n",
        "# Selanjutnya, dataset dibagi menjadi batch dengan ukuran BATCH_SIZE dan jika ada sisa data yang kurang dari BATCH_SIZE, data tersebut akan dihapus.\n",
        "# Terakhir, dataset di-prefetch untuk mengoptimalkan kinerja pelatihan.\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Menampilkan dataset yang telah diatur.\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPgQ4cdMkUqv",
        "outputId": "c500045d-6834-4bd0-b366-1ed5990796af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menetapkan ukuran vocab (jumlah kata unik) dari lapisan StringLookup.\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# Menetapkan dimensi embedding yang akan digunakan.\n",
        "embedding_dim = 256\n",
        "\n",
        "# Menetapkan jumlah unit dalam lapisan GRU (Gated Recurrent Unit).\n",
        "rnn_units = 1024\n",
        "\n",
        "# Membuat kelas model MyModel yang merupakan turunan dari tf.keras.Model.\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # Metode call digunakan untuk mendefinisikan aliran data dalam model.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "Le-TTjD0lPDl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat instance model MyModel dengan parameter-parameter yang telah ditetapkan sebelumnya.\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,  # Ukuran vocab (jumlah kata unik).\n",
        "    embedding_dim=embedding_dim,  # Dimensi embedding.\n",
        "    rnn_units=rnn_units  # Jumlah unit dalam lapisan GRU.\n",
        ")"
      ],
      "metadata": {
        "id": "8hBTeF8clTsy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengambil satu contoh batch dari dataset.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    # Menggunakan model untuk membuat prediksi dari input contoh batch.\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# Mencetak ringkasan (summary) model.\n",
        "model.summary()\n",
        "\n",
        "# Menggunakan tf.random.categorical untuk menghasilkan indeks karakter berikutnya dari distribusi prediksi.\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "# Mencetak input contoh batch dan prediksi karakter berikutnya.\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "jNsQK7Gylfun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f85d72-84f4-4c48-80da-44675f9aba87"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Input:\n",
            " b\"d this young prince agree,\\nI'll join mine eldest daughter and my joy\\nTo him forthwith in holy wedloc\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"!Cxty.YZYBMT--XvyqYfHHFPlJe'FufWOuXQ:tMi:KzfVK[UNK]!kWuE'jsG&?Tg;M$V? pYVKSDZlPiW\\ngYp'Vb.U?TxpZKxfFM!;.X\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan SparseCategoricalCrossentropy loss function.\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Menghitung mean loss dari contoh batch target dan prediksi.\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "\n",
        "# Menghitung e^mean_loss, yang dapat memberikan gambaran lebih intuitif tentang kebaikan model.\n",
        "tf.exp(example_batch_mean_loss).numpy()\n",
        "\n",
        "# Mengompilasi model dengan optimizer 'adam' dan loss yang telah ditentukan sebelumnya.\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Menentukan direktori tempat checkpoint akan disimpan.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Nama file checkpoint.\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# Membuat callback untuk ModelCheckpoint yang akan menyimpan bobot model pada setiap epoch.\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "WTeqkIQRlg3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63855e2e-2863-40c1-8ee4-3f50727b1e5b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1909504, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menetapkan jumlah epoch yang akan digunakan dalam pelatihan.\n",
        "EPOCHS = 20\n",
        "\n",
        "# Melakukan pelatihan model dengan dataset, dan menggunakan callback untuk menyimpan checkpoint.\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "he0kEjhnlsAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f437f6-9092-4220-da50-2bba264465b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 19/172 [==>...........................] - ETA: 15:04 - loss: 4.1204"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat kelas OneStep yang merupakan turunan dari tf.keras.Model.\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Membuat mask untuk mencegah generasi token \"[UNK]\".\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Memberikan nilai -inf pada setiap indeks yang tidak diinginkan.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Sesuaikan bentuk mask dengan ukuran vocab.\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Mengonversi string menjadi token ID.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Menjalankan model.\n",
        "    # predicted_logits.shape adalah [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Hanya menggunakan prediksi terakhir.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Menggunakan prediction mask: mencegah generasi token \"[UNK]\".\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Mengambil sample dari logits output untuk menghasilkan token ID.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Mengonversi dari token ID menjadi karakter.\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Mengembalikan karakter dan state model.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "93yBlzHJlyRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat instance model OneStep dengan menggunakan model yang telah dilatih, chars_from_ids, dan ids_from_chars.\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Membuat seed (awal) untuk generasi teks.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "# Melakukan iterasi untuk menghasilkan teks sepanjang 1000 karakter.\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "# Menggabungkan hasil generasi teks.\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "# Mencetak hasil generasi teks dan waktu eksekusi.\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\n",
        "# Melakukan generasi teks dengan seed yang berbeda.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "# Melakukan iterasi untuk menghasilkan teks sepanjang 1000 karakter.\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "# Menggabungkan hasil generasi teks.\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "# Mencetak hasil generasi teks dan waktu eksekusi.\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "PuC6KeYvl6Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor Model Generator\n",
        "\n",
        "# Menyimpan model OneStep ke direktori 'one_step'.\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "\n",
        "# Me-reload model yang telah disimpan.\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "\n",
        "# Inisialisasi state dan seed untuk generasi teks.\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "# Melakukan iterasi untuk menghasilkan teks sepanjang 100 karakter menggunakan model yang di-reload.\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "# Menggabungkan hasil generasi teks dan mencetaknya.\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "WtdnnD9Ql_DB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}